{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSeaAI\n",
    "#### Pipeline used for the cleaning and analysis of citizen science data with AI.\n",
    "\n",
    "This markdown will explain how to clean your citizen science data, and train a Yolov8 model on it. It has been developed with the intent to deal with DeepSeaSpy data (accessible from : https://ocean-spy.ifremer.fr/). \n",
    "\n",
    "Note : some functions may copy your images (specifically **vision**, **catalog**, **prepare_yolo**). Please be wary of the space available on your computer and act accordingly.\n",
    "\n",
    "### Contents\n",
    "You should have 2 python files and a Jupyter notebook file (download here):\n",
    "\n",
    "```\n",
    "deep-sea-lab\n",
    "├── Functions.py            <- Functions used for the cleaning/analysis.\n",
    "├── DeepSeaLab.ipynb        <- Here\n",
    "```\n",
    "\n",
    "**Functions** contains all the functions needed for the cleaning and analysis of citizen science datasets. Detailed explanations of the functions are to be found here. You can modify them and use them as the basis for your work. There are also functions that are useful just for the exploration of your dataset.\n",
    "\n",
    "**DeepSeaLab** is a ready-to-use file that you can change based on your needs/dataset. There are also examples on how to use the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "print(\"Python version:\", sys.version_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "Import the necessaries functions for the cleaning/analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, csv, json, collections, random, shutil\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from Functions import polygones2bb, points2bb, lines2bb, convert_yolo, prepare_yolo, vision, SaveCSV, create_yaml\n",
    "from Functions import unite, catalog, get_df\n",
    "import matplotlib.pyplot as plt\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access to data/files\n",
    "3 main paths are to be defined :\n",
    "\n",
    "```\n",
    "path_csv\n",
    "```\n",
    "Location of your dataset.\n",
    "\n",
    "\n",
    "```\n",
    "path_img\n",
    "```\n",
    "Path to the folder where your images are stored.\n",
    "the format of the images should be in .jpg\n",
    "\n",
    "```\n",
    "images\n",
    "├── image 1.jpg           \n",
    "├── image 2.jpg          \n",
    "├── ... \n",
    "```\n",
    "\n",
    "Then, finally\n",
    "```\n",
    "path_save\n",
    "```\n",
    "Where to store the cleaned dataset, catalogs, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv access :\n",
    "path_csv=\n",
    "# images :\n",
    "path_img=Path() \n",
    "# save\n",
    "path_save="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion of bounding boxes\n",
    "\n",
    "Our pipeline can convert 3 type of bounding boxes into regular bounding boxes.\n",
    "If your data already satisfies the following format, you can skip this part.\n",
    "\n",
    "|xmin |ymin |xmax |ymax |\n",
    "|-----|-----|-----|-----|\n",
    "|972  |982  |549  |559  |\n",
    "\n",
    "\n",
    "#### Polygons\n",
    "On DeepSeaSpy, polygons are in the json format :\n",
    "\n",
    "```\n",
    "[{\\x\\\":282,\\\"y\\\":115},{\\\"x\\\":15,\\\"y\\\":538},{\\\"x\\\":50,\\\"y\\\":679},{\\\"x\\\":285,\\\"y\\\":497}]\n",
    "```\n",
    "\n",
    "The column containing the polygon values can be named :\n",
    "|polygon_values                                                                         |\n",
    "|---------------------------------------------------------------------------------------|\n",
    "|[{\\x\\\":282,\\\"y\\\":115},{\\\"x\\\":15,\\\"y\\\":538},{\\\"x\\\":50,\\\"y\\\":679},{\\\"x\\\":285,\\\"y\\\":497}] |\n",
    "\n",
    "Therefore our pipeline was made to deal with such format. If you need to convert your own type of polygons, you can modify the way points are stored in the **polygons2bb** function.\n",
    "\n",
    "#### Lines\n",
    "Lines are to be in the following format, with two points defined by (x1,y1) and (x2,y2).\n",
    "\n",
    "|x1 |y1 |x2 |y2 |length|\n",
    "|---|---|---|---|------|\n",
    "|761|451|859|364|131   |\n",
    "\n",
    "Length is used to correct the converted bounding box, depending on the line's angle with the x axis. If the line is too vertical or too horizontal, the lines2bb function automatically corrects the converted bounding box. By default, if the angle is of +-5 degrees, the corrections happens. You can modify/find mor info in the Functions.py file.\n",
    "\n",
    "#### Points\n",
    "You can manually set a padding on the x and y axis in the Functions.py file.\n",
    "\n",
    "|x1 |y1 |\n",
    "|---|---|\n",
    "|761|451|\n",
    "\n",
    "The padding is the same for every point in your dataset, if you wish to use a different one for different species/uses, we recommend you split your dataset and run each part with a different padding. Then, you can concatenate all of your subsets with :\n",
    "\n",
    "```\n",
    "pd.concat([polybb,lignesbb,pointsbb])\n",
    "```\n",
    "\n",
    "We alsor recommend changing the names of your images columns and species columns, so that our functions can run properly.\n",
    "\n",
    "|name_img         |name_sp     |\n",
    "|-----------------|------------|\n",
    "|'MOMAR_90095.jpg'|'Buccinidae'|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your dataset\n",
    "data=\n",
    "data.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Rename data from DeepSeaSpy\n",
    "#data.rename(columns={'pos1x': 'x1', 'pos1y': 'y1','pos2x': 'x2', 'pos2y': 'y2','name_fr':'name_sp','name':'name_img'}, inplace=True)\n",
    "\n",
    "# Subset of polygon labels\n",
    "poly=\n",
    "# Subset of lines labels\n",
    "lines=\n",
    "# Subset of points labels\n",
    "points=\n",
    "\n",
    "# Polygons\n",
    "polybb=polygones2bb(poly)\n",
    "# Lines\n",
    "lignesbb=lines2bb(lines)\n",
    "# Points\n",
    "pointsbb=points2bb(points)\n",
    "\n",
    "# Concatenate your split dataset into a single one\n",
    "bb=pd.concat([polybb,lignesbb,pointsbb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the converted dataset\n",
    "SaveCSV(bb,path_save,'export_bb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb=pd.read_csv(os.path.join(path_save,'export_bb'), sep=None, engine='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision\n",
    "\n",
    "We encourage you to use the vision function, which will allow you to visualize your images with your bounding boxes added onto them.\n",
    "First, you have to define the object 'colors', which is a dictonary containing each species with its corresponding color coded in BGR.\n",
    "\n",
    "If you wish to, you can limit the number of saved images by adding 'nb_img' as an argument. \n",
    "```\n",
    "vision(bb,colors,path_img,path_save=None,nb_img=None)\n",
    "```\n",
    "When not specified, it saves the images in the parent directory of the path_img.\n",
    "\n",
    "This function copies the images from path_img, so it may generate a lot of data if you don't specify a number of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colors are in BGR\n",
    "couleurs = {\n",
    "        'Escargot buccinidé': (0, 0, 255),  # Red\n",
    "        'Couverture de moules': (255, 0, 0),  # Blue\n",
    "        'Couverture microbienne': (0, 255, 0),  # Green\n",
    "        'Couverture vers tubicole': (255, 255, 0),  \n",
    "        'Crabe araignée': (128, 0, 128),  \n",
    "        'Crabe bythograeidé': (255, 165, 0),  \n",
    "        'Crevette alvinocarididae': (255, 192, 203),  \n",
    "        'Escargot buccinidé': (0, 0, 255),  \n",
    "        'Ophiure': (139, 69, 19),  \n",
    "        'Poisson Cataetyx': (0, 255, 255),  \n",
    "        'Poisson chimère': (0, 0, 0),  \n",
    "        'Poisson zoarcidé': (192, 192, 192),  \n",
    "        'Pycnogonide': (255, 215, 0),  \n",
    "        'Ver polynoidé': (255, 255, 255),  \n",
    "        'Vers polynoidés': (245, 245, 220),\n",
    "        'Autre poisson':(245, 245, 220)\n",
    "    } \n",
    "\n",
    "vision(bb, couleurs, path_img, path_save, nb_img=10) #nb_img is optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unification of overlapping bounding boxes\n",
    "\n",
    "This step ensures that there is no redundancy in your dataset. You can skip this part if you are not dealing with this kind of problem.\n",
    "\n",
    "The unification of the bounding boxes is done when they are strictly overlapping (while the iou value is kept as None).\n",
    "Still, if you wish to limit the unification of the BB to a certain superposition threshold (iou), you can.\n",
    "\n",
    "```\n",
    "unite(dataframe, iou=None, grouper_0=False)\n",
    "```\n",
    "\n",
    "iou_thresh corresponds to the minimum Intersection over Union (IoU) value between two bounding boxes to consider them overlapping.\n",
    "\n",
    "Bounding boxes that are not overlapping any are automatically discarded. If you want to keep them, you can change the argument grouper_0 to grouper_0=True.\n",
    "\n",
    "The function keeps track of how many bounding boxes the final ones are made of in the column \"occurrences\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unification of all overlapping bounding boxes\n",
    "ubb=unite(bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping bounding boxes only if they are made of at least 3 overlapping ones\n",
    "ubb=ubb[ubb['occurences']>=3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unification only when bounding boxes are 0.2% overlapping\n",
    "ubb=unite(bb,0.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unification all of your bounding boxes, and not discarding isolated ones\n",
    "ubb=unite(bb,grouper_0=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save your dataframe\n",
    "SaveCSV(ubb,path_save,'ubb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ubb=pd.read_csv(r'C:\\Users\\alebeaud\\Desktop\\save_notebook\\ubb.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, csv, json, collections, random, shutil\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from Functions import polygones2bb, points2bb, lines2bb, convert_yolo, prepare_yolo, vision, SaveCSV, create_yaml\n",
    "from PIL import Image\n",
    "from Functions import unite, catalog, get_df\n",
    "import matplotlib.pyplot as plt\n",
    "os.getcwd()\n",
    "ubb=pd.read_csv(r'C:\\Users\\alebeaud\\Desktop\\save_notebook\\ubb.csv', sep=',')\n",
    "# csv access :\n",
    "path_csv=r'Q:/export_dss_20191018_Clean.csv'\n",
    "# images :\n",
    "path_img=Path(r'C:\\Users\\alebeaud\\Desktop\\Image_dsp') \n",
    "# save\n",
    "path_save=r'C:\\Users\\alebeaud\\Desktop\\save_notebook'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catalog\n",
    "\n",
    "Unite only unifies overlapping bounding boxes, it does not verify if the object you want to labelise is in fact inside the bounding box. If you wish to be very wary about which bounding box are to be kept in your dataset, you can use the two functions :\n",
    "\n",
    "```\n",
    "catalog(df, path_img, path_save=None)\n",
    "```\n",
    "When not specified, it saves the images in the parent directory of the path_img.\n",
    "\n",
    "Creates a catalog of snapshots from all the bounding boxes you have in your dataframe (df). You can then delete the snapshots of bounding boxes you want to discard.\n",
    "\n",
    "```\n",
    "get_df(df,path_save)\n",
    "```\n",
    "\n",
    "From the path_save (where your remaining snapshots are), and your unified dataframe (df), this function returns a dataframe that lists all the remaining bounding boxes from your own cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create snapshots from images and the dataframe\n",
    "catalog(ubb, path_img, path_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After discarding snapshots, get the remaining rows\n",
    "df=get_df(ubb,path_save)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for Yolo\n",
    "\n",
    "You can then use prepare_yolo to split your dataset in 3 (train,val,test), and train yolov8 on it.\n",
    "\n",
    "```\n",
    "prepare_yolo(df,path_save,path_img,prop=[.8,.1])\n",
    "```\n",
    "\n",
    "The **prop** parameter stands for proportion. It asks for the size of 2 subsets (in order) : **train** and **validation**. The remaining percentage is the size of the **test** subset. The test subset is not mandatory for the training of a model.\n",
    "prop=[.8,.1] means that the training subsets is 80% of our dataset, the validation subset is 10%. The remaining percentage is the test subset's size, here, 10%.\n",
    "\n",
    "Yolov8 takes bounding boxes in the following format :\n",
    "\n",
    "```\n",
    "class x y w h\n",
    "```\n",
    "\n",
    "With x and y the coordinates to the center of the bounding box. W and h are the width and height of the bounding box. Those 4 values are normalised between 0 and 1. prepare_yolo generates 1 txt file for each image, and each line in this file is the description of 1 bounding box.\n",
    "\n",
    "example :\n",
    "```\n",
    "7 0.5713542 0.6847222 0.0359375 0.0731481\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Size of your images\n",
    "size=[1920,1080]\n",
    "\n",
    "convert_yolo(df,size)\n",
    "prepare_yolo(df,path_save,path_img,prop=[.8,.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yolo training\n",
    "Now you can train a yolo model with your dataset.\n",
    "\n",
    "Your yolo training folder path should look like this :\n",
    "\n",
    "```\n",
    "yolo_training\n",
    "├── images        <- Where your images are\n",
    "|   ├── train\n",
    "|   ├── val\n",
    "|   ├── test\n",
    "├── labels        <- Where your bounding boxes/labels for each image are\n",
    "|   ├── train\n",
    "|   ├── val\n",
    "|   ├── test\n",
    "```\n",
    "Yolo needs a yaml file to understands where the data is, and what the classes are.\n",
    "You can create the file yourself or use create_yaml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a yaml file containing all of the information necessary for running Yolov8 on your data\n",
    "create_yaml(path_save,df,'output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is ready for Yolov8.\n",
    "\n",
    "\n",
    "If you installed our requirements correctly, you can train your Yolov8 model in python :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a new YOLO model from scratch\n",
    "model = YOLO('yolov8n.yaml')  # build a new model from YAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or load a pretrained YOLO model (recommended for training)\n",
    "model = YOLO('yolov8n.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get where the .yaml file is stored\n",
    "output=str('output'+'.yaml')\n",
    "yaml_path=os.path.join(path_save,output)\n",
    "\n",
    "# Train the model\n",
    "results = model.train(data=yaml_path, epochs=10, imgsz=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model to identify objects\n",
    "# Load your trained YOLOv8n model\n",
    "model = YOLO('your/path/to/trained/model/here')\n",
    "img_to_predict=r'C:\\Users\\alebeaud\\Desktop\\images_to_be_predicted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_img=list(img_to_predict.glob('**/*.jpg'))\n",
    "\n",
    "# Run inference on your list of images\n",
    "results=model(list_img)\n",
    "\n",
    "# Process results list\n",
    "for result in results:\n",
    "    boxes = result.boxes  # Boxes object for bounding box outputs\n",
    "    masks = result.masks  # Masks object for segmentation masks outputs\n",
    "    keypoints = result.keypoints  # Keypoints object for pose outputs\n",
    "    probs = result.probs  # Probs object for classification outputs\n",
    "    obb = result.obb  # Oriented boxes object for OBB outputs\n",
    "    result.show()  # display to screen\n",
    "    result.save(filename=\"result.jpg\")  # save to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also launch the training within a command terminal (same line of code for windows or linux). This can be useful if you don't want to open a python interactive window, or you are working remotly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the training\n",
    "!yolo task=detect mode=train model=yolov8n.yaml imgsz=640 data=absolute/path/to/data_yolo.yaml show_labels=False epochs=10 batch=8 name=run1\n",
    "\n",
    "# Once your model is trained, you can run 'predict' to detect objects\n",
    "!yolo predict model=path/to/yolo/runs/detect/run1/weights/best.pt source=path/to/data show_labels=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to go further\n",
    "\n",
    "If you wish to train the hyperparameters of your trained model, you can do so.\n",
    "This step is to be done only if you have the allowable resources to do so, as hyperparameter tuning takes a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('/runs/detect/buccin_cit_07/weights/best.pt')\n",
    "\n",
    "model.tune(data='buccins_cit_07.yaml',epochs=200, iterations=300, optimizer='AdamW', plots=True, save=True, val=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Species Detection\n",
    "\n",
    "Provided by Ifremer, iMagine.\n",
    "\n",
    "[![Build Status](https://jenkins.services.ai4os.eu/buildStatus/icon?job=AI4OS-hub/deep-species-detection/main)](https://jenkins.services.ai4os.eu/job/AI4OS-hub/job/deep-species-detection/job/main/)\n",
    "\n",
    "# Citizen science and data cleaning\n",
    "In this repository, you will find a pipeline that cleans citizen science image datasets, and automatically trains a YoloV8 model on it.\n",
    "You may also use this module to run inference on a pre trained YoloV8 model, specifically on 2 species : Buccinidae and Bythograeidae.\n",
    "The pipeline converts bounding boxes from Deep Sea Spy format (lines, points, polygons) to regular bounding boxes (xmin, xmax, ymin, ymax). The conversion step is optional. It then unifies overlapping bounding boxes of each species, using the redundancy of citizen identifications as a \n",
    "There is 3 ways to use the pipeline :\n",
    "    - DeepSeaLab.ipynb : step by step guide to clean the dataset and launch the Yolov8 training\n",
    "    - Pipeline_txt.py : automatically cleans the dataset and launches the Yolov8 training with the arguments stored in config.txt\n",
    "    - Deepaas API : easily visualize, customize and monitor the Yolov8 training\n",
    "\n",
    "## Project structure\n",
    "\n",
    "```\n",
    "├── Jenkinsfile             <- Describes basic Jenkins CI/CD pipeline\n",
    "├── LICENSE                 <- License file\n",
    "├── README.md               <- The top-level README for developers using this project.\n",
    "├── VERSION                 <- Version file indicating the version of the model\n",
    "│\n",
    "├── deep-sea-lab            <- All of the data cleaning files\n",
    "│   ├── DeepSeaLab.ipynb    <- Notebook pipeline for data cleaning & Yolov8 training\n",
    "│   ├── Functions.py        <- Data processing file DeepSeaLab draws function from\n",
    "│   ├── Pipeline_txt.py     <- Automatic pipeline to clean the data & train Yolov8\n",
    "│   ├── config.txt          <- Configuration file for Pipeline.txt, which stores arguments to run the pipeline\n",
    "│\n",
    "├── yolov8\n",
    "│   ├── README.md           <- Instructions on how to integrate your model with DEEPaaS.\n",
    "│   ├── __init__.py         <- Makes <your-model-source> a Python module\n",
    "│   ├── ...                 <- Other source code files\n",
    "│   └── config.py           <- Module to define CONSTANTS used across the AI-model python package\n",
    "│\n",
    "├── api                     <- API subpackage for the integration with DEEP API\n",
    "│   ├── __init__.py         <- Makes api a Python module, includes API interface methods\n",
    "│   ├── config.py           <- API module for loading configuration from environment\n",
    "│   ├── responses.py        <- API module with parsers for method responses\n",
    "│   ├── schemas.py          <- API module with definition of method arguments\n",
    "│   └── utils.py            <- API module with utility functions\n",
    "│\n",
    "├── data                    <- Data subpackage for the integration with DEEP API\n",
    "│   ├── external            <- Data from third party sources.\n",
    "│   ├── processed           <- The final, canonical data sets for modeling.\n",
    "│   └── raw                 <- The original, immutable data dump.\n",
    "│\n",
    "├── docs                   <- A default Sphinx project; see sphinx-doc.org for details\n",
    "│\n",
    "├── models                 <- Folder to store your models\n",
    "│\n",
    "├── notebooks              <- Jupyter notebooks. Naming convention is a number (for ordering),\n",
    "│                             the creator's initials (if many user development),\n",
    "│                             and a short `_` delimited description, e.g.\n",
    "│                             `1.0-jqp-initial_data_exploration.ipynb`.\n",
    "│\n",
    "├── references             <- Data dictionaries, manuals, and all other explanatory materials.\n",
    "│\n",
    "├── reports                <- Generated analysis as HTML, PDF, LaTeX, etc.\n",
    "│   └── figures            <- Generated graphics and figures to be used in reporting\n",
    "│\n",
    "├── requirements-dev.txt    <- Requirements file to install development tools\n",
    "├── requirements-test.txt   <- Requirements file to install testing tools\n",
    "├── requirements.txt        <- Requirements file to run the API and models\n",
    "│\n",
    "├── pyproject.toml         <- Makes project pip installable (pip install -e .)\n",
    "│\n",
    "├── tests                   <- Scripts to perform code testing\n",
    "│   ├── configurations      <- Folder to store the configuration files for DEEPaaS server\n",
    "│   ├── conftest.py         <- Pytest configuration file (Not to be modified in principle)\n",
    "│   ├── data                <- Folder to store the data for testing\n",
    "│   ├── models              <- Folder to store the models for testing\n",
    "│   ├── test_deepaas.py     <- Test file for DEEPaaS API server requirements (Start, etc.)\n",
    "│   ├── test_metadata       <- Tests folder for model metadata requirements\n",
    "│   ├── test_predictions    <- Tests folder for model predictions requirements\n",
    "│   └── test_training       <- Tests folder for model training requirements\n",
    "│\n",
    "└── tox.ini                <- tox file with settings for running tox; see tox.testrun.org\n",
    "```\n",
    "\n",
    "## Data requirements\n",
    "\n",
    "The data required for the pipeline is to have a folder with your images, and a .csv file containing all of your annotations.\n",
    "\n",
    "If your dataset is incomplete, it is better to remove incomplete rows rather than . Missing images will not cause problems with the pipeline.\n",
    "For the conversion step, this pipeline converts data from Deep Sea Spy format :\n",
    "\n",
    "|shapes  |x1 |y1 |x2 |y2 |polygon_values|name_img|species    |\n",
    "|--------|---|---|---|---|--------------|--------|-----------|\n",
    "|point   |59 |34 |NaN|NaN|NaN           |4366.jpg|Pycnogonid |\n",
    "|lines   |761|451|859|364|NaN           |4366.jpg|Buccinidae |\n",
    "|polygons|NaN|NaN|NaN|NaN|[{\\x\":282,\"y\":115},{\"x\":15,\"y\":538},{\"x\":50,\"y\":679},{\"x\":285,\"y\":497}]|4366.jpg|Mussels coverage|\n",
    "\n",
    "To a regular format :\n",
    "\n",
    "|xmin |xmax |ymin |ymax |name_img|species    |\n",
    "|-----|-----|-----|-----|--------|-----------|\n",
    "|69   |49   |24   |44   |4366.jpg|Pycnogonid |\n",
    "|761  |859  |364  |451  |4366.jpg|Buccinidae |\n",
    "|15   |285  |115  |679  |4366.jpg|Mussels coverage|\n",
    "\n",
    "If your data is already in this format, you can skip the conversion steps (more details in the cleaning sections).\n",
    "\n",
    "The pipeline expects image resolution of 1920x1080. You can input images with a different size by changing width_images and height_images in the beginning of Functions.py. If you have images of varying resolutions, you can modify the functions so that they take in the type of image you have.\n",
    "\n",
    "\n",
    "# Cleaning from DeepSeaLab.ipynb\n",
    "The **Notebook** is a ready-to-use, step by step cleaning file that you can change based on your needs/dataset. This option is better for a first use of the module since the notebook brings more context and guidance to the cleaning steps.\n",
    "You can skip the conversion steps by skipping the cells that won't help your case. Arguments are pre filled to show you what's expected from the user.\n",
    "You can launch the notebook by double clicking the file on the left, inside the deep-sea-lab folder.\n",
    "\n",
    "# Cleaning from Pipeline_txt.py\n",
    "Python script that automatically runs all the functions needed for the cleaning and analysis of citizen science datasets.\n",
    "This option is more straight forward than using the python notebook. Detailed explanations of the functions can be found in the file in itself. You can modify them and use them as the basis for your work.\n",
    "This file uses arguments in the config.txt file, which are pre filled to show you what's expected from the user.\n",
    "You can modify arguments based on your needs.\n",
    "### Arguments and usage\n",
    "Paths to your data is required as the first arguments in the config.txt file\n",
    "```\n",
    "# Paths\n",
    "# csv access :\n",
    "path_csv=/storage/export.csv\n",
    "# images :\n",
    "path_imgs=/storage/Image_dsp/\n",
    "# save\n",
    "path_save=/storage/save\n",
    "```\n",
    "Those paths are by default, we are expecting you to have connected your Nextcloud account to the iMagine platform.\n",
    "path_imgs should refer to the folder containing all of your images\n",
    "In the config.txt file, if your dataset only contains annotated lines, it is expected :\n",
    "```\n",
    "# Dataset options\n",
    "polygons=false\n",
    "points=false\n",
    "lines=True\n",
    "```\n",
    "If your dataset only contains annotated polygons and points, it is expected :\n",
    "```\n",
    "# Dataset options\n",
    "polygons=True\n",
    "points=True\n",
    "lines=false\n",
    "```\n",
    "If your dataset is already in the regular format cited in the data requirements (therefore, you do not need the data conversion), you can put everything in false :\n",
    "```\n",
    "# Dataset options\n",
    "polygons=false\n",
    "points=false\n",
    "lines=false\n",
    "```\n",
    "The pipeline will still create the training dataset from your data, and will train YoloV8 on it.\n",
    "\n",
    "In the config.txt file, you can change the YoloV8 training parameters. They are set by their default value (from https://github.com/ultralytics/ultralytics).\n",
    "You may also change the hyperparameters, but it is recommended to do so only if you know what each modified argument does to the training step.\n",
    "If you wish to train the hyperparameters on a specific model, you can do so by running the last cell in the DeepSeaLab Notebook.\n",
    "\n",
    "# Running Deepaas for YoloV8 training and inference\n",
    "\n",
    "First, install the package :\n",
    "\n",
    "```\n",
    "git clone https://github.com/ai4os-hub/deep-species-detection\n",
    "cd  deep-species-detection\n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "You can launch DeepaaS by using :\n",
    "\n",
    "```\n",
    "deepaas-run --listen-ip 0.0.0.0\n",
    "```\n",
    "\n",
    "# Adding DeepaaS API into the existing codebase\n",
    "In this repository, we have integrated a DeepaaS API into the  Ultralytics YOLOv8, enabling the seamless utilization of this pipeline. The inclusion of the DeepaaS API enhances the functionality and accessibility of the code, making it easier for users to leverage and interact with the pipeline efficiently.\n",
    "\n",
    "\n",
    "\n",
    "><span style=\"color:Blue\">**Note:**</span> Before installing the API, please make sure to install the following system packages: `gcc`, `libgl1`, and `libglib2.0-0` as well. These packages are essential for a smooth installation process and proper functioning of the framework.\n",
    "```\n",
    "apt update\n",
    "apt install -y gcc\n",
    "apt install -y libgl1\n",
    "apt install -y libglib2.0-0\n",
    "```\n",
    "\n",
    "\n",
    "# Environment variables settings\n",
    "\"In `./api/config.py` you can configure several environment variables:\n",
    "\n",
    "- `DATA_PATH`: Path definition for the data folder; the default is './data'.\n",
    "- `MODELS_PATH`: Path definition for saving trained models; the default is './models'.\n",
    "- `REMOTE_PATH`: Path to the remote directory containing your trained models. Rclone uses this path for downloading or listing the trained models.\n",
    "- `YOLOV8_DEFAULT_TASK_TYPE`: Specify the default tasks related to your work among detection (det), segmentation (seg), and classification (cls).\n",
    "- `YOLOV8_DEFAULT_WEIGHTS`: Define default timestamped weights for your trained models to be used during prediction. If no timestamp is specified by the user during prediction, the first model in YOLOV8_DEFAULT_WEIGHTS will be used. If it is set to None, the Yolov8n trained on coco/imagenet will be used. Format them as timestamp1, timestamp2, timestamp3, ...\"\n",
    "\n",
    "# Track your experiments with Mlfow\n",
    "If you want to use Mflow to track and log your experiments, you should first set the following environment variables:\n",
    "- `MLFLOW_TRACKING_URI`\n",
    "- `MLFLOW_TRACKING_USERNAME`\n",
    "- `MLFLOW_TRACKING_PASSWORD`\n",
    "- `MLFLOW_EXPERIMENT_NAME` (for the first experiment)\n",
    "\n",
    "optional options:\n",
    "\n",
    "- `MLFLOW_RUN`\n",
    "- `MLFLOW_RUN_DESCRIPTION`\n",
    "- `MLFLOW_AUTHOR`\n",
    "- `MLFLOW_MODEL_NAME`: This name will be used as the name for your model registered in the MLflow Registry.\n",
    "- Then you should set the argument `Enable_MLFLOW` to `True` during the execution of the training.\n",
    "\n",
    "\n",
    "# Dataset Preparation\n",
    "- Detection (det), oriented bounding boxes detection (obb) and Segmentation Tasks (seg):\n",
    "\n",
    "    - To train the yolov8 model, your annotations should be saved as yolo formats (.txt). Please organize your data in the following structure:\n",
    "```\n",
    "\n",
    "│\n",
    "└── my_dataset\n",
    "    ├──  train\n",
    "    │    ├── imgs\n",
    "    │    │   ├── img1.jpg\n",
    "    │    │   ├── img2.jpg\n",
    "    │    │   ├── ...\n",
    "    │    ├── labels\n",
    "    │    │   ├── img1.txt\n",
    "    │    │   ├── img2.txt\n",
    "    │    │   ├── ...\n",
    "    │    \n",
    "    ├── val    \n",
    "    │    ├── imgs\n",
    "    │    │   ├── img_1.jpg\n",
    "    │    │   ├── img_2.jpg\n",
    "    │    │   ├── ...\n",
    "    │    ├── labels\n",
    "    │    │   ├── img_1.txt\n",
    "    │    │   ├── img_2.txt\n",
    "    │    │   ├── ...\n",
    "    │    \n",
    "    ├── test    \n",
    "    │    ├── imgs\n",
    "    │    │   ├── img_1.jpg\n",
    "    │    │   ├── img_2.jpg\n",
    "    │    │   ├── ...\n",
    "    │    ├── labels\n",
    "    │    │   ├── img_1.txt\n",
    "    │    │   ├── img_2.txt\n",
    "    │    │   ├── ...\n",
    "    │    \n",
    "    └── config.yaml\n",
    "```\n",
    "\n",
    "The `config.yaml` file contains the following information about the data:\n",
    "\n",
    "```yaml\n",
    "# Images and labels directory should be insade 'fasterrcnn_pytorch_api/data' directory.\n",
    "train: 'path/to/my_dataset/train/imgs'\n",
    "val: 'path/to/my_dataset/val/imgs'\n",
    "test: 'path/to/my_dataset/test/imgs' #optional\n",
    "# Class names.\n",
    "names: \n",
    "    0: class1, \n",
    "    1: class2,\n",
    "     ...\n",
    "\n",
    "# Number of classes.\n",
    "NC: n\n",
    "```\n",
    "The `train` and `val` fields specify the paths to the directories containing the training and validation images, respectively.\n",
    "`names` is a dictionary of class names. The order of the names should match the order of the object class indices in the YOLO dataset files.\n",
    "\n",
    "><span style=\"color:Blue\">**Note:**</span>The train and val path should be a complete path or relative from\n",
    "data directory e.g. `root/path/to/mydata/train/images` or if it is in the `path/to/deep-species-detection/data/raw` just \n",
    "`mydata/train/images`\n",
    "\n",
    "\n",
    "-  Classification Task (cls):\n",
    "For the classification task, the dataset format should be as follows:\n",
    "```\n",
    "data/\n",
    "|-- class1/\n",
    "|   |-- img1.jpg\n",
    "|   |-- img2.jpg\n",
    "|   |-- ...\n",
    "|\n",
    "|-- class2/\n",
    "|   |-- img1.jpg\n",
    "|   |-- img2.jpg\n",
    "|   |-- ...\n",
    "|\n",
    "|-- class3/\n",
    "|   |-- img1.jpg\n",
    "|   |-- img2.jpg\n",
    "|   |-- ...\n",
    "|\n",
    "|-- ...\n",
    "```\n",
    "><span style=\"color:Blue\">**Note:**</span>  For the classification task, you don't need the config.yaml file. Simply provide the path to the data directory in the data argument for training.\n",
    "\n",
    "><span style=\"color:Blue\">**Note:**</span>  If you have annotations files in Coco json format or Pascal VOC xml format, you can use the following script to convert them to the proper format for yolo. \n",
    "``` \n",
    "deep-species-detection/yolov8/seg_coco_json_to_yolo.py #for segmentation\n",
    "deep-species-detection/yolov8/preprocess_ann.py #For detection\n",
    "``` \n",
    "# Available Models\n",
    "\n",
    "The Ultralytics YOLOv8 model can be used to train multiple tasks including classification, detection, and segmentatio.\n",
    "To train the model based on your project, you can select on of the task_type option in the training arguments and the corresponding model will be loaded and trained.\n",
    "for each task, you can select the model arguments among the following options:\n",
    "\n",
    "``` \n",
    "\"yolov8n.yaml\",\n",
    "\"yolov8n.pt\",\n",
    "\"yolov8s.yaml\",\n",
    "\"yolov8s.pt\",\n",
    "\"yolov8m.yaml\",\n",
    "\"yolov8m.pt\",\n",
    "\"yolov8l.yaml\",\n",
    "\"yolov8l.pt\",\n",
    "\"yolov8x.yaml\",\n",
    "\"yolov8x.pt\",\n",
    "```\n",
    "`yolov8X.yaml` bulid a model from scratch and\n",
    "`yolov8X.pt` load a pretrained model (recommended for training).\n",
    "\n",
    "# Launching the API\n",
    "\n",
    "To train the model, run:\n",
    "```\n",
    "deepaas-run --listen-ip 0.0.0.0\n",
    "```\n",
    "Then, open the Swagger interface, change the hyperparameters in the train section, and click on train.\n",
    "\n",
    "><span style=\"color:Blue\">**Note:**</span>  Please note that the model training process may take some time depending on the size of your dataset and the complexity of your custom backbone. Once the model is trained, you can use the API to perform inference on new images.\n",
    "\n",
    "><span style=\"color:Blue\">**Note:**</span> Augmentation Settings:\n",
    "among the training arguments, there are options related to augmentation, such as flipping, scaling, etc. The default values are set to automatically activate some of these options during training. If you want to disable augmentation entirely or partially, please review the default values and adjust them accordingly to deactivate the desired augmentations.\n",
    "\n",
    "# Inference Methods\n",
    "\n",
    "You can utilize the Swagger interface to upload your images or videos and obtain the following outputs:\n",
    "\n",
    "- For images:\n",
    "\n",
    "    - An annotated image highlighting the object of interest with a bounding box.\n",
    "    - A JSON string providing the coordinates of the bounding box, the object's name within the box, and the confidence score of the object detection.\n",
    "\n",
    "- For videos:\n",
    "\n",
    "    - A video with bounding boxes delineating objects of interest throughout.\n",
    "    - A JSON string accompanying each frame, supplying bounding box coordinates, object names within the boxes, and confidence scores for the detected objects.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepSeaAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
